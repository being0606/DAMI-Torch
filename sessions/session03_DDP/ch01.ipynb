{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "# On Windows platform, the torch.distributed package only\n",
    "# supports Gloo backend, FileStore and TcpStore.\n",
    "# For FileStore, set init_method parameter in init_process_group\n",
    "# to a local file. Example as follow:\n",
    "# init_method=\"file:///f:/libtmp/some_file\"\n",
    "# dist.init_process_group(\n",
    "#    \"gloo\",\n",
    "#    rank=rank,\n",
    "#    init_method=init_method,\n",
    "#    world_size=world_size)\n",
    "# For TcpStore, same way as on Linux.\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.net1 = nn.Linear(10, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.net2 = nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net2(self.relu(self.net1(x)))\n",
    "\n",
    "\n",
    "def demo_basic(rank, world_size):\n",
    "    print(f\"Running basic DDP example on rank {rank}.\")\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    # create model and move it to GPU with id rank\n",
    "    model = ToyModel().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(rank)\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    cleanup()\n",
    "    print(f\"Finished running basic DDP example on rank {rank}.\")\n",
    "\n",
    "\n",
    "def run_demo(demo_fn, world_size):\n",
    "    mp.spawn(demo_fn,\n",
    "             args=(world_size,),\n",
    "             nprocs=world_size,\n",
    "             join=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def demo_checkpoint(rank, world_size):\n",
    "    print(f\"Running DDP checkpoint example on rank {rank}.\")\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    model = ToyModel().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "\n",
    "    CHECKPOINT_PATH = tempfile.gettempdir() + \"/model.checkpoint\"\n",
    "    if rank == 0:\n",
    "        # All processes should see same parameters as they all start from same\n",
    "        # random parameters and gradients are synchronized in backward passes.\n",
    "        # Therefore, saving it in one process is sufficient.\n",
    "        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)\n",
    "\n",
    "    # Use a barrier() to make sure that process 1 loads the model after process\n",
    "    # 0 saves it.\n",
    "    dist.barrier()\n",
    "    # configure map_location properly\n",
    "    map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n",
    "    ddp_model.load_state_dict(\n",
    "        torch.load(CHECKPOINT_PATH, map_location=map_location, weights_only=True))\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(rank)\n",
    "\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Not necessary to use a dist.barrier() to guard the file deletion below\n",
    "    # as the AllReduce ops in the backward pass of DDP already served as\n",
    "    # a synchronization.\n",
    "\n",
    "    if rank == 0:\n",
    "        os.remove(CHECKPOINT_PATH)\n",
    "\n",
    "    cleanup()\n",
    "    print(f\"Finished running DDP checkpoint example on rank {rank}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ToyMpModel(nn.Module):\n",
    "    def __init__(self, dev0, dev1):\n",
    "        super(ToyMpModel, self).__init__()\n",
    "        self.dev0 = dev0\n",
    "        self.dev1 = dev1\n",
    "        self.net1 = torch.nn.Linear(10, 10).to(dev0)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.net2 = torch.nn.Linear(10, 5).to(dev1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.dev0)\n",
    "        x = self.relu(self.net1(x))\n",
    "        x = x.to(self.dev1)\n",
    "        return self.net2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def demo_model_parallel(rank, world_size):\n",
    "    print(f\"Running DDP with model parallel example on rank {rank}.\")\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    # setup mp_model and devices for this process\n",
    "    dev0 = rank * 2\n",
    "    dev1 = rank * 2 + 1\n",
    "    mp_model = ToyMpModel(dev0, dev1)\n",
    "    ddp_mp_model = DDP(mp_model)\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # outputs will be on dev1\n",
    "    outputs = ddp_mp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(dev1)\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    cleanup()\n",
    "    print(f\"Finished running DDP with model parallel example on rank {rank}.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    assert n_gpus >= 2, f\"Requires at least 2 GPUs to run, but got {n_gpus}\"\n",
    "    world_size = n_gpus\n",
    "    run_demo(demo_basic, world_size)\n",
    "    run_demo(demo_checkpoint, world_size)\n",
    "    world_size = n_gpus//2\n",
    "    run_demo(demo_model_parallel, world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.net1 = nn.Linear(10, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.net2 = nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net2(self.relu(self.net1(x)))\n",
    "\n",
    "\n",
    "def demo_basic():\n",
    "    torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n",
    "    dist.init_process_group(\"nccl\")\n",
    "    rank = dist.get_rank()\n",
    "    print(f\"Start running basic DDP example on rank {rank}.\")\n",
    "    # create model and move it to GPU with id rank\n",
    "    device_id = rank % torch.cuda.device_count()\n",
    "    model = ToyModel().to(device_id)\n",
    "    ddp_model = DDP(model, device_ids=[device_id])\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(device_id)\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "    dist.destroy_process_group()\n",
    "    print(f\"Finished running basic DDP example on rank {rank}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo_basic()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
